# Proyecto de Web Scraping para XYZ Corp

## Descripci√≥n del Proyecto üìÑ

XYZ Corp busca una frase que represente sus valores y misi√≥n. Para ello, se desarroll√≥ un programa en Python que realiza web scraping en [Quotes to Scrape](https://quotes.toscrape.com/). El objetivo es extraer todas las frases, junto con el autor, los tags asociados y la p√°gina "about" de cada autor. Los datos obtenidos se formatearon y almacenaron adecuadamente.

### Objetivos del Proyecto üéØ

- **Acceso a la web**: Extraer datos de una web preparada para el scraping.
- **Extracci√≥n de informaci√≥n relevante**: Obtener frases, autores, tags y p√°ginas "about".
- **Formateo de datos**: Asegurar que los datos est√©n limpios y organizados.
- **Almacenamiento en base de datos**: Guardar la informaci√≥n extra√≠da en un formato estructurado (CSV).

## Soluci√≥n Implementada üõ†Ô∏è

Para desarrollar la soluci√≥n, se bas√≥ en dos ayudas principales, integr√°ndolas y complement√°ndolas para cumplir con los requisitos del proyecto.

### C√≥digo Base Utilizado

**Primera Ayuda**: Se utiliz√≥ el siguiente c√≥digo proporcionado para el scraping de libros, adapt√°ndolo a la nueva tarea:

```python
# Instalaci√≥n de dependencias
"""
!pip install requests
!pip install BeautifulSoup4
!pip install pandas
"""

# Importaci√≥n de librer√≠as
import requests
import pandas as pd
from bs4 import BeautifulSoup
import csv

# Variable con la direcci√≥n del sitio a hacer scraping
url = "https://books.toscrape.com/"

# Realizar la petici√≥n con request y guardamos el contenido de la pagina
books_to_scrape = requests.get(url)

# Parsear la informaci√≥n, mediante una instancia de beautiful soup
soup = BeautifulSoup(books_to_scrape.text, 'lxml')
# Confirmar el tipo de dato de la "sopa" creada
type(soup)

# Imprimir con formato
print(soup.prettify())

# Apuntar a los art√≠culos para extraerlos
articulos = soup.find_all('article', attrs={'class':'product_pod'})
articulos

# Lista temporal para guardar los libros
libros = []

# Iterar los articulos y obtener las variables
for articulo in articulos:
    titulo_libro = articulo.h3.a.get('title')
    imagen_libro = articulo.img.get('src')
    precio_libro_raw = articulo.find('p', attrs={'class':'price_color'}).get_text()[1:]
    precio_libro = precio_libro_raw[1:]
    divisa = precio_libro_raw[0]
    temp_row = [titulo_libro, imagen_libro, precio_libro, divisa]
    libros.append(temp_row)
    print(temp_row)

quotes_df = pd.DataFrame(libros, columns=['titulo_libro', 'imagen_libro', 'precio_libro', 'divisa'])

# Inspeccionar el dataframe
first_10 = quotes_df.head(10)

print(first_10)
```


### Desarrollo del Scraper de Frases

**Segunda Ayuda**: Se utiliz√≥ el c√≥digo desarrollado con la asistencia de ChatGPT, adapt√°ndolo para extraer frases, autores, tags y p√°ginas "about" de Quotes to Scrape.

### Tecnolog√≠as Utilizadas üíª

- **Git/GitHub**: Control de versiones y colaboraci√≥n en el proyecto.
- **Trello**: Gesti√≥n y seguimiento de las tareas.
- **BeautifulSoup** y Requests: Web scraping.
- **Pandas y CSV**: Manejo y almacenamiento de datos.
- **OpenOffice**: Visualizaci√≥n de los datos extra√≠dos.



## Conclusi√≥n üèÅ
Este proyecto alcanz√≥ el nivel esencial, cumpliendo con los requisitos b√°sicos de extracci√≥n y almacenamiento de datos. Las herramientas y t√©cnicas utilizadas permitieron desarrollar un programa funcional que satisface las necesidades de XYZ Corp. Se document√≥ todo el proceso y el c√≥digo est√° disponible en el repositorio de GitHub para su revisi√≥n.

Para m√°s detalles, consulta el c√≥digo fuente y la documentaci√≥n en el repositorio GitHub.
